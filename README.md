# **Cybersecurity Alert Classification using BERT & XGBoost**

## **Overview**

This project presents a lightweight and flexible machine learning pipeline for classifying cybersecurity alerts using a combination of BERT transformer-based embeddings and XGBoost.

Unlike large-scale models such as GPTs, this approach is optimized for efficiency and ease of deployment, making it suitable for practical, resource-constrained environments while still leveraging the benefits of deep contextual understanding through domain-adaptive pretraining (DAPT) of BERT on cybersecurity-specific text.

The pipeline integrates:

    - BERT (DAPT) to generate semantic embeddings from textual alert data.
    - Engineered categorical features from structured fields.
    - XGBoost to handle high-dimensional input and missing values.

Why XGBoost?
Cybersecurity datasets often have noisy, incomplete, and feature-rich data. XGBoost is designed to handle such complexity—during training, it automatically learns the optimal path for instances with missing values at each decision node, allowing the model to make informed predictions even when some data is absent.

### **Features / Data Structure**

**The input data contains the following features: (These fields are just for example. If you have more data, please train on that as well.)** 

- `Alert Details` → Text
- `Analysis` → Text
- `Incident Area` → Category
- `Recommendation` → Text
- `Severity` → Category

**Enriched fields from Alert details and Analysis:**

- `Domains`
- `Emails`
- `Filenames`
- `Hash`
- `IP`
- `URLs`

These text fields are derived from prior security alert data and investigations carried out by analysts or security tools like XDR, SOAR, and others. To ensure the BERT model understands the nuances of cybersecurity language and context, it's recommended to train it on historical cybersecurity datasets — whether labeled or unlabeled — to capture the patterns, terminology, and structure unique to this domain.

## **Steps / Pipeline**

### **Data Ingestion**

- Load raw alert data from an Excel file.

### **IOC Enrichment**

- Extract Indicators of Compromise (IOCs) from alert details and summary fields for further enrichment.

### **Data Cleaning**

- Sanitize the data by handling and removing null or missing values to ensure data quality.

### **Train-Test Split**

- Split the dataset into training and testing subsets for unbiased model evaluation.

### **BERT DAPT Model Fine-tuning**

- Use a BERT transformer model fine-tuned on a cybersecurity corpus (`contentfile.txt`) — Domain Adaptive PreTraining (DAPT).
- Fine-tuned model is saved in the `dapt_model` directory.

### **Categorical Feature Handling**

- Manually identify categorical features (can be automated as needed).
- Encode categories using `HashEncoder`.

### **Pipeline Creation**

- Develop a preprocessing pipeline that combines categorical encoding and other data transformations.

### **Model Training**

- Train an XGBoost classifier using the processed features.
- Apply Recursive Feature Elimination with Cross-Validation (RFECV) for feature selection.

### **Evaluation**

- Evaluate model performance using accuracy and other relevant metrics.

## **Usage**

### **Requirements**
- Python 3.7+
- pandas
- scikit-learn
- xgboost
- transformers (BERT)
- category_encoders

This project leverages a hybrid machine learning pipeline combining BERT (Domain-Adaptive Pre-Training - DAPT) and XGBoost to perform classification on cybersecurity-related textual data.

By default, the model uses the dataset generated by the get-iocs.py script. This script performs pattern-based IOC extraction from textual columns within your dataset, creating a processed dataset suitable for model training.

### Input Data
You should provide a dataset with raw security alert text (e.g., from XDR/SOAR/SIEM). The script will extract IOCs such as:
- `Domains`
- `Emails`
- `Filenames`
- `Hash`
- `IP`
- `URLs` etc

### Model Training Strategy

If you have two datasets:
- One without target labels (unlabeled)
- One with target labels (labeled)

We recommend:
- Pre-training the BERT (DAPT) model on the unlabeled dataset to adapt the transformer to the cybersecurity domain.
- Fine-tuning the adapted BERT model on the labeled dataset for the final classification task using XGBoost.

This approach helps the BERT model learn the language and structure of cybersecurity data, improving performance during training and testing phases.

### **Future Improvements**

- Expand IOC enrichment with additional threat intelligence feeds.
- Implement hyperparameter tuning for XGBoost.
- Deploy the model as an API for real-time alert classification.
